import os
import sys
import streamlit as st
import nest_asyncio

# Streamlitì—ì„œ ë¹„ë™ê¸° ì‘ì—…ì„ ìœ„í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì •
nest_asyncio.apply()

# âœ… pysqlite3 íŒ¨ì¹˜ (ChromaDBìš©)
__import__('pysqlite3')
sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')

# âœ… LangChain ë° ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory
from langchain_chroma import Chroma


# âœ… Gemini API í‚¤ ì„¤ì •
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”!")
    st.stop()

# âœ… PDF ê²½ë¡œ ë° ê³ ìœ  ë²¡í„°DB ê²½ë¡œ ì§€ì •
PDF_PATH = r"/mount/src/librarychatbot_gemini/ì•ˆì „í•œ ë°”ë‹¤ì—¬í–‰_ìµœì¢….pdf"
PDF_NAME = os.path.splitext(os.path.basename(PDF_PATH))[0]
VECTOR_DIR = f"./chroma_db_{PDF_NAME}"  # PDFë§ˆë‹¤ ê³ ìœ  í´ë”

# âœ… Streamlit ìºì‹œ ì´ˆê¸°í™” ì˜µì…˜
if st.button("ğŸ”„ ìºì‹œ ë° ì„ë² ë”© ë°ì´í„° ì´ˆê¸°í™”"):
    if os.path.exists(VECTOR_DIR):
        import shutil
        shutil.rmtree(VECTOR_DIR)
        st.success("âœ… ì´ì „ ChromaDB ë°ì´í„° ì‚­ì œ ì™„ë£Œ!")
    st.cache_resource.clear()
    st.success("âœ… Streamlit ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ! ì•±ì„ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")

# âœ… PDF ë¡œë“œ ë° ë¶„í• 
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()

# âœ… ChromaDB ìƒì„±
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(_docs)
    st.info(f"ğŸ“„ {len(split_docs)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¡œ ë¶„í• í–ˆìŠµë‹ˆë‹¤.")

    st.info("ğŸ¤– ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘... (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ)")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ë²¡í„° ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...")
    vectorstore = Chroma.from_documents(
        split_docs,
        embeddings,
        persist_directory=VECTOR_DIR
    )
    st.success("ğŸ’¾ ìƒˆë¡œìš´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ!")
    return vectorstore

# âœ… ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
@st.cache_resource
def get_vectorstore(_docs):
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    if os.path.exists(VECTOR_DIR):
        st.info("ğŸ“‚ ê¸°ì¡´ ì•ˆì „í•œ ë°”ë‹¤ì—¬í–‰ ë²¡í„°DB ë¡œë“œ ì¤‘...")
        return Chroma(persist_directory=VECTOR_DIR, embedding_function=embeddings)
    else:
        return create_vector_store(_docs)

# âœ… ì „ì²´ ì´ˆê¸°í™” (RAG ì²´ì¸)
@st.cache_resource
def initialize_components(selected_model):
    pages = load_and_split_pdf(PDF_PATH)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ì§ˆë¬¸ ì¬êµ¬ì„±ìš© í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = """Given a chat history and the latest user question \
    which might reference context in the chat history, formulate a standalone question \
    which can be understood without the chat history. Do NOT answer the question, \
    just reformulate it if needed and otherwise return it as is."""
    contextualize_q_prompt = ChatPromptTemplate.from_messages([
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    # ì§ˆë¬¸-ë‹µë³€ìš© í”„ë¡¬í”„íŠ¸
    qa_system_prompt = """You are an assistant for question-answering tasks. \
    Use the following pieces of retrieved context to answer the question. \
    If you don't know the answer, just say that you don't know. \
    Keep the answer short, accurate, and polite. \
    Please answer in Korean and use emojis naturally with your answer. \
    {context}"""
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", qa_system_prompt),
        MessagesPlaceholder("history"),
        ("human", "{input}"),
    ])

    llm = ChatGoogleGenerativeAI(
        model=selected_model,
        temperature=0.7,
        convert_system_message_to_human=True
    )

    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

# âœ… Streamlit UI
st.header("ğŸŒŠ ì•ˆì „í•œ ë°”ë‹¤ì—¬í–‰ Q&A ì±—ë´‡ ğŸ’¬")

if not os.path.exists(VECTOR_DIR):
    st.info("ğŸ”„ ì²« ì‹¤í–‰ì…ë‹ˆë‹¤. PDFë¥¼ ì„ë² ë”© ì¤‘ì…ë‹ˆë‹¤... (ì•½ 5ë¶„ ì†Œìš”)")
else:
    st.info(f"ğŸ“‚ '{PDF_NAME}' ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤!")

option = st.selectbox(
    "Select Gemini Model",
    ("gemini-2.0-flash-exp", "gemini-2.5-flash", "gemini-2.0-flash-lite"),
    index=0,
    help="Gemini 2.0 Flashê°€ ê°€ì¥ ë¹ ë¥´ê³  íš¨ìœ¨ì ì…ë‹ˆë‹¤"
)

try:
    with st.spinner("ğŸ”§ ì±—ë´‡ ì´ˆê¸°í™” ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”"):
        rag_chain = initialize_components(option)
    st.success("âœ… ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    st.stop()

chat_history = StreamlitChatMessageHistory(key="chat_messages")
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

if "messages" not in st.session_state:
    st.session_state["messages"] = [{
        "role": "assistant",
        "content": "ì•ˆì „í•œ ë°”ë‹¤ì—¬í–‰ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”! ğŸŒŠğŸ˜Š"
    }]

for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.chat_message("human").write(prompt_message)
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            config = {"configurable": {"session_id": "safe_sea_chat"}}
            response = conversational_rag_chain.invoke({"input": prompt_message}, config)
            answer = response['answer']
            st.write(answer)
            with st.expander("ğŸ“˜ ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc in response['context']:
                    st.markdown(doc.metadata['source'], help=doc.page_content)
