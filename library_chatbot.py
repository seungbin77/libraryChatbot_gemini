import os
import sys
import streamlit as st
import nest_asyncio

# Streamlit ë¹„ë™ê¸° ì¶©ëŒ ë°©ì§€
nest_asyncio.apply()

# ================================
# 1. LangChain & Chroma ì„í¬íŠ¸
# ================================
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.output_parsers import StrOutputParser

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever

from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

# ğŸ”¥ ìµœì‹  Chroma Settings ì‚¬ìš© (dict ì ˆëŒ€ ì‚¬ìš© X)
from langchain_chroma import Chroma, Settings


# ================================
# 2. Google Gemini API Key
# ================================
try:
    os.environ["GOOGLE_API_KEY"] = st.secrets["GOOGLE_API_KEY"]
except Exception:
    st.error("âš ï¸ GOOGLE_API_KEYë¥¼ Streamlit Secretsì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
    st.stop()


# ================================
# 3. PDF ì„¤ì •
# ================================
PDF_PATH = r"/mount/src/librarychatbot_gemini/ì•ˆì „í•œ ë°”ë‹¤ì—¬í–‰_ìµœì¢….pdf"
PDF_NAME = os.path.splitext(os.path.basename(PDF_PATH))[0]

VECTOR_DIR = f"./chroma_db_{PDF_NAME}"


# ================================
# 4. Streamlit UI â€” ìºì‹œ ì´ˆê¸°í™”
# ================================
if st.button("ğŸ”„ ChromaDB / ìºì‹œ ì´ˆê¸°í™”"):
    import shutil
    if os.path.exists(VECTOR_DIR):
        shutil.rmtree(VECTOR_DIR)
    st.cache_resource.clear()
    st.success("â™»ï¸ ì´ˆê¸°í™” ì™„ë£Œ! ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.")


# ================================
# 5. PDF ë¡œë“œ & ë¶„í• 
# ================================
@st.cache_resource
def load_and_split_pdf(file_path):
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()


# ================================
# 6. Chroma Settings (í•„ìˆ˜!!)
# ================================
def get_chroma_settings():
    return Settings(
        chroma_db_impl="duckdb+parquet",
        persist_directory=VECTOR_DIR,
        anonymized_telemetry=False
    )


# ================================
# 7. ChromaDB ìƒì„±
# ================================
@st.cache_resource
def create_vector_store(_docs):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200
    )
    split_docs = text_splitter.split_documents(_docs)

    st.info(f"ğŸ“„ {len(split_docs)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ.")

    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    st.info("ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")

    vectorstore = Chroma.from_documents(
        documents=split_docs,
        embedding=embeddings,
        collection_name="default",
        client_settings=get_chroma_settings()
    )

    st.success("ğŸ’¾ ChromaDB ì €ì¥ ì™„ë£Œ!")
    return vectorstore


# ================================
# 8. ê¸°ì¡´ DB ë¡œë“œ or ìƒì„±
# ================================
@st.cache_resource
def get_vectorstore(_docs):
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )

    if os.path.exists(VECTOR_DIR):
        st.info("ğŸ“‚ ê¸°ì¡´ ChromaDB ë¡œë“œ ì¤‘...")
        return Chroma(
            persist_directory=VECTOR_DIR,
            embedding_function=embeddings,
            collection_name="default",
            client_settings=get_chroma_settings()
        )
    else:
        return create_vector_store(_docs)


# ================================
# 9. RAG ì´ˆê¸°í™”
# ================================
@st.cache_resource
def initialize_components(selected_model):
    pages = load_and_split_pdf(PDF_PATH)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ---- ì§ˆë¬¸ ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸ ----
    contextualize_prompt = ChatPromptTemplate.from_messages([
        ("system", 
        """
        Given the chat history and the latest user question, rewrite it as a standalone question. 
        Do NOT answer.
        """),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    # ---- QA í”„ë¡¬í”„íŠ¸ ----
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system",
        """
        You are an assistant for question-answering tasks.
        Use the retrieved context.
        If you donâ€™t know the answer, say you don't know.
        Answer in Korean with emojis.
        {context}
        """),
        MessagesPlaceholder("history"),
        ("human", "{input}")
    ])

    llm = ChatGoogleGenerativeAI(
        model=selected_model,
        temperature=0.7,
        convert_system_message_to_human=True
    )

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_prompt
    )

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

    rag_chain = create_retrieval_chain(
        history_aware_retriever, question_answer_chain
    )

    return rag_chain


# ================================
# 10. UI
# ================================
st.header("ğŸŒŠ ì•ˆì „í•œ ë°”ë‹¤ì—¬í–‰ Q&A ì±—ë´‡ ğŸ’¬")

if not os.path.exists(VECTOR_DIR):
    st.info("ğŸ”„ ì²« ì‹¤í–‰ â€” ë²¡í„° ë°ì´í„° ìƒì„± ì¤‘ì…ë‹ˆë‹¤.")
else:
    st.info(f"ğŸ“‚ '{PDF_NAME}' ë²¡í„°DB ì¤€ë¹„ë¨!")


selected_model = st.selectbox(
    "Select Gemini model",
    ("gemini-2.0-flash-exp", "gemini-2.5-flash", "gemini-2.0-flash-lite")
)


# RAG ë¡œë“œ
try:
    with st.spinner("ğŸ”§ ì‹œìŠ¤í…œ ë¡œë”© ì¤‘..."):
        rag_chain = initialize_components(selected_model)
    st.success("âœ¨ ì¤€ë¹„ ì™„ë£Œ!")
except Exception as e:
    st.error(f"âš ï¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    st.stop()


# ================================
# 11. ëŒ€í™” íˆìŠ¤í† ë¦¬ ë¶ˆëŸ¬ì˜¤ê¸°
# ================================
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversation_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer"
)


# ================================
# 12. ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
# ================================
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)


# ================================
# 13. ì‚¬ìš©ì ì…ë ¥
# ================================
if user_input := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.chat_message("human").write(user_input)

    with st.chat_message("ai"):
        with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
            response = conversation_chain.invoke(
                {"input": user_input},
                config={"configurable": {"session_id": "sea_chat"}}
            )

            st.write(response["answer"])

            # ì°¸ê³  ë¬¸ì„œ í‘œì‹œ
            if "context" in response:
                with st.expander("ğŸ“˜ ì°¸ê³  ë¬¸ì„œ"):
                    for doc in response["context"]:
                        st.markdown(doc.metadata.get("source", "ì¶œì²˜ ì—†ìŒ"))
